# PonderTTT: ë°©ë²•ë¡  ì•„ì¹´ì´ë¸Œ

**ì‘ì„±ì¼**: 2025-11-09
**ëª©ì **: ì½”ë“œ ì¬ì‘ì„± ì „ í•µì‹¬ ì•„ì´ë””ì–´ ë° ë°©ë²•ë¡  ë³´ì¡´
**ì£¼ì˜**: ì´ ë¬¸ì„œëŠ” ìˆœìˆ˜í•˜ê²Œ ì•„ì´ë””ì–´ì™€ ë°©ë²•ë¡ ë§Œ ê¸°ë¡í•©ë‹ˆë‹¤.

---

## Executive Summary

PonderTTTëŠ” Test-Time Training (TTT) ì•„í‚¤í…ì²˜ì— **ì ì‘í˜• ê³„ì‚° í• ë‹¹**ì„ ì ìš©í•˜ëŠ” ì—°êµ¬ì…ë‹ˆë‹¤. í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ê° í† í°ì˜ **ë‚œì´ë„ì— ë”°ë¼ gradient descent ë°˜ë³µ íšŸìˆ˜ë¥¼ ë™ì ìœ¼ë¡œ ì¡°ì •**í•˜ì—¬ ê³„ì‚° íš¨ìœ¨ì„±ì„ ë†’ì´ëŠ” ê²ƒì…ë‹ˆë‹¤.

**í•µì‹¬ ì§ˆë¬¸**: "ëª¨ë“  í† í°ì´ ë™ì¼í•œ ê³„ì‚°ëŸ‰ì„ í•„ìš”ë¡œ í•˜ëŠ”ê°€?"

**ê°€ì„¤**: ë‚œì´ë„ê°€ ë†’ì€ í† í°ì€ ë” ë§ì€ ë°˜ë³µì´ í•„ìš”í•˜ê³ , ì‰¬ìš´ í† í°ì€ ì ì€ ë°˜ë³µìœ¼ë¡œ ì¶©ë¶„í•˜ë‹¤.

**ì ‘ê·¼ë²•**: ê°•í™”í•™ìŠµ(REINFORCE)ì„ ì‚¬ìš©í•˜ì—¬ í† í°ë³„ ìµœì  ë°˜ë³µ íšŸìˆ˜ë¥¼ í•™ìŠµí•œë‹¤.

**âš ï¸ ì£¼ìš” ìš°ë ¤ì‚¬í•­**:
1. ğŸ”´ **Policy overhead**: BiLSTM (14.2M params) ë¹„ìš©ì´ ì ˆê°ë¶„ ìƒì‡„ ê°€ëŠ¥ â†’ Net gain ë¶ˆí™•ì‹¤
2. ğŸ”´ **ë¹„í˜„ì‹¤ì  ê¸°ëŒ€**: 15-30% ì ˆê°ì€ ê³¼ë„í•¨ â†’ 0-5% ë˜ëŠ” negative ì˜ˆìƒ
3. âš ï¸ **WikiText-2 ë„ˆë¬´ ì‘ìŒ**: 2M tokens â†’ WikiText-103 (100M) í•„ìš”
4. âš ï¸ **REINFORCE ë¶ˆì•ˆì •**: High variance â†’ Training ì–´ë ¤ì›€
5. âš ï¸ **Null hypothesis ê°€ëŠ¥ì„±**: ê°€ì„¤ ìì²´ê°€ í‹€ë¦´ ìˆ˜ ìˆìŒ â†’ Negative resultë„ í•™ìˆ ì  ê¸°ì—¬

**í˜„ì‹¤ì  ëª©í‘œ**: Interpretability í™•ë³´ (ì„±ê³µ), Efficiency gainì€ ë³´ë„ˆìŠ¤ (ë¶ˆí™•ì‹¤)

---

## 1. ì—°êµ¬ ë™ê¸° (Motivation)

### 1.1 ë°°ê²½: Test-Time Training (TTT)

**TTTì˜ í•µì‹¬ ê°œë…**:
- Hidden stateë¥¼ í•™ìŠµ ê°€ëŠ¥í•œ ëª¨ë¸ë¡œ ì·¨ê¸‰
- ê° í† í° ì²˜ë¦¬ ì‹œ self-supervised learningìœ¼ë¡œ ëª¨ë¸ ì—…ë°ì´íŠ¸
- ì—…ë°ì´íŠ¸ëœ ëª¨ë¸ì´ ë‹¤ìŒ í† í°ì˜ hidden stateê°€ ë¨

**TTTì˜ ì´ì **:
- ê¸´ ë¬¸ë§¥ ì²˜ë¦¬ ëŠ¥ë ¥
- ì„ í˜• ë³µì¡ë„ (Self-attentionì€ quadratic)
- Test-time adaptation

**TTTì˜ í•œê³„**:
- ëª¨ë“  í† í°ì— ëŒ€í•´ ë™ì¼í•œ ê³„ì‚°ëŸ‰ ì‚¬ìš©
- í† í° ë‚œì´ë„ë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠìŒ
- ê³„ì‚° ìì›ì˜ ë¹„íš¨ìœ¨ì  ì‚¬ìš© ê°€ëŠ¥ì„±

### 1.2 ë™ê¸°: ì ì‘í˜• ê³„ì‚° í• ë‹¹

**ê´€ì°°**:
- ëª¨ë“  í† í°ì´ ë™ì¼í•œ ë‚œì´ë„ë¥¼ ê°€ì§€ì§€ ì•ŠìŒ
- ì‰¬ìš´ í† í° (ì˜ˆ: "the", "a"): ì ì€ ê³„ì‚°ìœ¼ë¡œ ì¶©ë¶„
- ì–´ë ¤ìš´ í† í° (ì˜ˆ: ì „ë¬¸ìš©ì–´, ë§¥ë½ ì˜ì¡´ì  ë‹¨ì–´): ë” ë§ì€ ê³„ì‚° í•„ìš”

**ì•„ì´ë””ì–´**:
- í† í°ë³„ë¡œ gradient descent ë°˜ë³µ íšŸìˆ˜ Kë¥¼ ë™ì ìœ¼ë¡œ í• ë‹¹
- ë‚œì´ë„ ë†’ì€ í† í° â†’ í° K (ì˜ˆ: K=8)
- ë‚œì´ë„ ë‚®ì€ í† í° â†’ ì‘ì€ K (ì˜ˆ: K=1)

**ê¸°ëŒ€ íš¨ê³¼**:
- ì „ì²´ ê³„ì‚°ëŸ‰ì€ ìœ ì§€í•˜ë©´ì„œ ì„±ëŠ¥ í–¥ìƒ, ë˜ëŠ”
- ì„±ëŠ¥ì€ ìœ ì§€í•˜ë©´ì„œ ê³„ì‚°ëŸ‰ ì ˆê°

---

## 2. í•µì‹¬ ë°©ë²•ë¡ 

### 2.1 ë¬¸ì œ ì •ì˜

**ì…ë ¥**: í† í° ì‹œí€€ìŠ¤ X = [xâ‚, xâ‚‚, ..., xâ‚™]

**ëª©í‘œ**: ê° í† í° xâ‚œì— ëŒ€í•´ ìµœì  ë°˜ë³µ íšŸìˆ˜ Kâ‚œ âˆˆ {1, 2, 4, 8} ê²°ì •

**ì œì•½ì¡°ê±´**:
- í‰ê·  ë°˜ë³µ íšŸìˆ˜ê°€ ëª©í‘œ ê°’ (ì˜ˆ: K_avg = 4) ìœ ì§€
- ì „ì²´ FLOPs ì˜ˆì‚° ì´ˆê³¼ ê¸ˆì§€

**ìµœì í™” ëª©í‘œ**:
```
Maximize: ì–¸ì–´ ëª¨ë¸ ì„±ëŠ¥ (Minimize perplexity)
Subject to: E[Kâ‚œ] â‰¤ K_target
```

### 2.2 ì ì‘í˜• ê³„ì‚° í• ë‹¹ í”„ë ˆì„ì›Œí¬

#### 2.2.1 Iterative Gradient Descent

**ê¸°ë³¸ ì•„ì´ë””ì–´**:
- ê° í† í°ì— ëŒ€í•´ fast-weight ëª¨ë¸ Wë¥¼ Kâ‚œë²ˆ ì—…ë°ì´íŠ¸
- Self-supervised loss: L = ||W @ K - (V - K)||Â²

**ì—…ë°ì´íŠ¸ ê·œì¹™**:
```
For token t:
  Wâ‚€ = W from previous token
  For k = 1 to Kâ‚œ:
    Compute loss: L = ||W_{k-1} @ K_t - (V_t - K_t)||Â²
    Compute gradient: âˆ‡_W L
    Update: W_k = W_{k-1} - Î· * âˆ‡_W L

  Use W_{Kâ‚œ} for output computation
```

**Sequential carry-over**:
- Token tì˜ ìµœì¢… weight W_{Kâ‚œ}ê°€ token t+1ì˜ ì´ˆê¸° weight Wâ‚€ê°€ ë¨
- ì´ì „ í† í°ì˜ í•™ìŠµì´ ë‹¤ìŒ í† í°ì— ì˜í–¥

#### 2.2.2 í•™ìŠµëœ Halting Policy

**Policy Network êµ¬ì¡°**:
- ì…ë ¥: í† í°ì˜ hidden state representation
- ì¶œë ¥: K âˆˆ {1, 2, 4, 8}ì— ëŒ€í•œ í™•ë¥  ë¶„í¬

**Architecture**:
1. **Context Encoder**: Bidirectional LSTMìœ¼ë¡œ local context í¬ì°©
2. **Step Predictor**: 2-layer MLPë¡œ K ì˜ˆì¸¡

**Policy output**:
```
Ï€_Î¸(K_t | h_t) = Categorical distribution over {1, 2, 4, 8}
```

### 2.3 ê°•í™”í•™ìŠµ: REINFORCE Algorithm

#### 2.3.1 ê¸°ë³¸ REINFORCE

**Objective**:
```
J(Î¸) = E_{K ~ Ï€_Î¸}[R(K)]

where R(K) = -Loss(K)  (reward is negative loss)
```

**Policy Gradient**:
```
âˆ‡_Î¸ J(Î¸) = E[âˆ‡_Î¸ log Ï€_Î¸(K_t | h_t) * (R_t - b_t)]

where:
- R_t: reward at position t
- b_t: baseline (moving average of rewards)
```

**í•™ìŠµ ì•Œê³ ë¦¬ì¦˜**:
1. Policy Ï€_Î¸ë¡œ K ìƒ˜í”Œë§ (stochastic during training)
2. ìƒ˜í”Œë§ëœ Kë¡œ forward pass ìˆ˜í–‰
3. Per-token loss ê³„ì‚°
4. Policy gradientë¡œ Î¸ ì—…ë°ì´íŠ¸

#### 2.3.2 Temporal Credit Assignment

**í•µì‹¬ ì•„ì´ë””ì–´**: Token tì˜ K ê²°ì •ì´ **ë¯¸ë˜ í† í°ë“¤ì˜ ì„±ëŠ¥ì—ë„ ì˜í–¥**

**ì´ìœ **:
- Fast-weightê°€ sequentialí•˜ê²Œ carry-overë¨
- Token tì—ì„œ í° K ì‚¬ìš© â†’ ë” ë‚˜ì€ W í•™ìŠµ
- ë” ë‚˜ì€ W â†’ Token t+1, t+2, ...ì˜ ì„±ëŠ¥ í–¥ìƒ

**Monte Carlo Returns**:
```
G_t = Î£_{i=t}^{T} Î³^{i-t} * r_i

where:
- r_i: reward at position i (negative loss)
- Î³: discount factor (ì˜ˆ: 0.99)
- G_t: discounted cumulative return
```

**Policy Gradient with Temporal Credit**:
```
âˆ‡_Î¸ J(Î¸) = E[âˆ‡_Î¸ log Ï€_Î¸(K_t | h_t) * (G_t - b_t)]
```

**Interpretation**:
- Î³=0: per-token rewardë§Œ ê³ ë ¤ (myopic)
- Î³=0.99: ë¯¸ë˜ 100 í† í°ê¹Œì§€ ì˜í–¥ ê³ ë ¤ (far-sighted)

#### 2.3.3 Compute Budget Constraint

**ë¬¸ì œ**: Policyê°€ í•­ìƒ K=8ì„ ì„ íƒí•  ìˆ˜ ìˆìŒ

**í•´ê²°**: Regularization term ì¶”ê°€

**Objective with constraint**:
```
J(Î¸) = E[R(K)] - Î» * |E[K] - K_target|

where:
- Î»: compute penalty coefficient (ì˜ˆ: 0.01)
- K_target: ëª©í‘œ í‰ê·  K (ì˜ˆ: 4.0)
```

**Interpretation**:
- E[K] > K_target: penalty ë¶€ê³¼ (ë„ˆë¬´ ë§ì€ ê³„ì‚° ì‚¬ìš©)
- E[K] < K_target: penalty ì—†ìŒ (íš¨ìœ¨ì )

### 2.4 ë‚œì´ë„ ì¸¡ì • (Heuristic Baselines)

í•™ìŠµëœ policyì™€ ë¹„êµí•˜ê¸° ìœ„í•œ heuristic ê¸°ë°˜ ë°©ë²•ë“¤:

#### 2.4.1 Entropy-based Policy

**ì•„ì´ë””ì–´**: ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„±ì´ ë†’ì€ í† í°ì´ ì–´ë ¤ì›€

**ë‚œì´ë„ ì¸¡ì •**:
```
difficulty(x_t) = H(p(x_{t+1} | x_{â‰¤t}))

where H(p) = -Î£ p(x) log p(x)
```

**K í• ë‹¹**:
- High entropy â†’ K=8
- Medium entropy â†’ K=4 or K=2
- Low entropy â†’ K=1

#### 2.4.2 Loss-based Policy

**ì•„ì´ë””ì–´**: í˜„ì¬ ëª¨ë¸ì´ ì˜ˆì¸¡í•˜ê¸° ì–´ë ¤ìš´ í† í°ì´ ì–´ë ¤ì›€

**ë‚œì´ë„ ì¸¡ì •**:
```
difficulty(x_t) = -log p(x_t | x_{<t})  (cross-entropy loss)
```

**K í• ë‹¹**: Loss ê°’ì— ë”°ë¼ threshold-based assignment

#### 2.4.3 Gradient-based Policy

**ì•„ì´ë””ì–´**: Gradient magnitudeê°€ í° í† í°ì´ ì–´ë ¤ì›€

**ë‚œì´ë„ ì¸¡ì •**:
```
difficulty(x_t) = ||âˆ‡_W L(x_t)||
```

#### 2.4.4 Calibration

**ë¬¸ì œ**: Heuristic ê°’ì˜ ë²”ìœ„ê°€ ë°ì´í„°ì— ë”°ë¼ ë‹¤ë¦„

**í•´ê²°**: Validation setì—ì„œ percentile-based threshold ê³„ì‚°

**ì•Œê³ ë¦¬ì¦˜**:
1. Validation setì—ì„œ ëª¨ë“  í† í°ì˜ difficulty ê³„ì‚°
2. Percentile ê³„ì‚° (ì˜ˆ: 25%, 50%, 75%)
3. Threshold ì„¤ì •:
   - difficulty < p25 â†’ K=1
   - p25 â‰¤ difficulty < p50 â†’ K=2
   - p50 â‰¤ difficulty < p75 â†’ K=4
   - difficulty â‰¥ p75 â†’ K=8

---

## 3. ì‹¤í—˜ ì„¤ê³„

### 3.1 ë¹„êµ ë°©ë²•ë¡ 

**Baseline methods**:
1. **Uniform K=1**: ëª¨ë“  í† í°ì— K=1 (minimal compute)
2. **Uniform K=2**: ëª¨ë“  í† í°ì— K=2
3. **Uniform K=4**: ëª¨ë“  í† í°ì— K=4 (reference)
4. **Uniform K=8**: ëª¨ë“  í† í°ì— K=8 (maximum compute)

**Heuristic methods**:
5. **Entropy-based**: Entropyë¡œ K ê²°ì •
6. **Loss-based**: Lossë¡œ K ê²°ì •
7. **Gradient-based**: Gradient normìœ¼ë¡œ K ê²°ì •

**Learned methods**:
8. **Learned (Î»=0.01, target=4)**: REINFORCE with compute constraint
9. **Learned (Î»=0.05, target=4)**: Stronger compute penalty
10. **Learned (Î»=0.01, no target)**: No compute constraint

### 3.2 í‰ê°€ ì§€í‘œ

**Primary metrics**:
- **Perplexity**: ì–¸ì–´ ëª¨ë¸ ì„±ëŠ¥ (lower is better)
- **FLOPs**: ê³„ì‚° ë¹„ìš© (lower is better)

**Pareto frontier**:
- Xì¶•: FLOPs
- Yì¶•: Perplexity
- ëª©í‘œ: Pareto optimal ë‹¬ì„±

**Statistical tests**:
- Multiple seeds (10+)
- Paired t-test
- Bonferroni correction
- Confidence intervals

### 3.3 ì„±ê³µ ê¸°ì¤€

**Hypothesis**: í•™ìŠµëœ adaptive policyê°€ uniform baselineë³´ë‹¤ ìš°ìˆ˜

**ì„±ê³µ ì¡°ê±´** (ë³´ìˆ˜ì  ì„¤ì •):
1. **Quality maintenance**: Perplexityê°€ Uniform-K4 ëŒ€ë¹„ 1% ì´ë‚´
2. **Efficiency gain**: Net FLOPs (policy overhead í¬í•¨) ê°€ 5% ì´ìƒ ì ˆê°
3. **Statistical significance**: p < 0.05 (Bonferroni corrected)
4. **Difficulty correlation**: Optimal Kì™€ difficulty metric ê°„ r > 0.3

**âš ï¸ ì£¼ì˜**: Policy overheadë¡œ ì¸í•´ net gainì´ 0%ì¼ ê°€ëŠ¥ì„±ë„ ê³ ë ¤í•´ì•¼ í•¨

### 3.4 Oracle Analysis

**ëª©ì **: Adaptive allocationì˜ upper bound ì¶”ì •

**ë°©ë²•**:
1. ê° í† í°ì— ëŒ€í•´ K âˆˆ {1,2,4,8} ëª¨ë‘ ì‹œë„
2. Per-token loss ì¸¡ì •
3. ê° í† í°ì˜ optimal K ì„ íƒ

**ë¶„ì„**:
- Oracle K distribution
- Oracle performance (best possible)
- Learned policyê°€ oracleì— ì–¼ë§ˆë‚˜ ê·¼ì ‘í•˜ëŠ”ê°€?

---

## 4. ì£¼ìš” ì—°êµ¬ ì§ˆë¬¸

### RQ1: Adaptive allocationì´ íš¨ê³¼ì ì¸ê°€?

**ì§ˆë¬¸**: "ë™ì¼í•œ ê³„ì‚° ì˜ˆì‚°ìœ¼ë¡œ adaptiveê°€ uniformë³´ë‹¤ ë‚˜ì€ê°€?"

**ì‹¤í—˜**:
- Uniform K=4 vs Learned (target=4)
- FLOPs ë§¤ì¹­ ì‹œ perplexity ë¹„êµ

**Expected**: Learned < Uniform (lower perplexity)

### RQ2: ì–´ë–¤ ë‚œì´ë„ metricì´ ê°€ì¥ ì¢‹ì€ê°€?

**ì§ˆë¬¸**: "Entropy, loss, gradient ì¤‘ ë¬´ì—‡ì´ optimal Kì™€ ê°€ì¥ ìƒê´€ê´€ê³„ê°€ ë†’ì€ê°€?"

**ì‹¤í—˜**:
- Oracle K ê³„ì‚°
- ê° metricê³¼ì˜ correlation ì¸¡ì •

**Expected**: Loss-based metricì´ ê°€ì¥ ë†’ì€ correlation

### RQ3: Temporal credit assignmentê°€ ë„ì›€ì´ ë˜ëŠ”ê°€?

**ì§ˆë¬¸**: "Î³=0.99 (temporal) vs Î³=0.0 (per-token) ì¤‘ ë¬´ì—‡ì´ ë‚˜ì€ê°€?"

**ì‹¤í—˜**:
- Learned with Î³=0.99
- Learned with Î³=0.0
- Performance ë¹„êµ

**Expected**: Î³=0.99ê°€ ë” ë‚˜ìŒ (sequential dependency ê³ ë ¤)

### RQ4: Compute constraintì˜ ì˜í–¥ì€?

**ì§ˆë¬¸**: "Î» ê°’ì´ quality-efficiency tradeoffì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ëŠ”ê°€?"

**ì‹¤í—˜**:
- Î» âˆˆ {0.0, 0.01, 0.05, 0.1}
- Pareto curve ê·¸ë¦¬ê¸°

**Expected**: Î»ê°€ í´ìˆ˜ë¡ ë‚®ì€ FLOPs, ì•½ê°„ ë†’ì€ perplexity

---

## 5. ê´€ë ¨ ì—°êµ¬ì™€ì˜ ê´€ê³„

### 5.1 Adaptive Computation Time (ACT)

**ACT (Graves 2016)**:
- RNNì—ì„œ ê° í† í°ì˜ **thinking time** ë™ì  ì¡°ì •
- Halting probability í•™ìŠµ

**PonderTTT vs ACT**:
- ìœ ì‚¬: ë‘˜ ë‹¤ adaptive compute allocation
- ì°¨ì´:
  - ACT: Layer depth ì¡°ì • (ë™ì¼ layerë¥¼ ì—¬ëŸ¬ ë²ˆ)
  - PonderTTT: Test-time gradient descent ë°˜ë³µ íšŸìˆ˜ ì¡°ì •

### 5.2 Mixture of Depths (MoD)

**MoD (Raposo et al., 2024)**:
- ì¼ë¶€ í† í°ë§Œ full transformer layer í†µê³¼
- ë‚˜ë¨¸ì§€ëŠ” skip connection

**PonderTTT vs MoD**:
- ìœ ì‚¬: í† í°ë³„ compute ì°¨ë³„í™”
- ì°¨ì´:
  - MoD: Layer ì „ì²´ë¥¼ skipí• ì§€ ê²°ì •
  - PonderTTT: TTT ë‚´ë¶€ì˜ ë°˜ë³µ íšŸìˆ˜ ì¡°ì •

### 5.3 SIFT (Efficient Data Selection)

**SIFT (Mindermann et al., 2022)**:
- Training ì¤‘ ì–´ë ¤ìš´ dataì— ì§‘ì¤‘
- Data weighting

**PonderTTT vs SIFT**:
- ìœ ì‚¬: Difficulty-aware resource allocation
- ì°¨ì´:
  - SIFT: Data selection (training)
  - PonderTTT: Per-token compute (inference)

### 5.4 Official TTT

**Official TTT (Sun et al., 2024)**:
- Test-time training with self-supervised learning
- Linear complexity

**PonderTTT vs Official TTT**:
- ìœ ì‚¬: ë‘˜ ë‹¤ TTT ì‚¬ìš©
- ì°¨ì´:
  - Official: ëª¨ë“  í† í° ë™ì¼ ì²˜ë¦¬
  - PonderTTT: í† í°ë³„ adaptive compute

**PonderTTTì˜ ê¸°ì—¬**: Official TTT ìœ„ì— adaptive compute ë ˆì´ì–´ ì¶”ê°€

### 5.5 ì°¨ë³„ì  ìš”ì•½

| Method | Allocation Target | Our Contribution |
|--------|------------------|------------------|
| ACT | Layer depth | TTT iteration count |
| MoD | Layer skip/compute | Fine-grained iteration control |
| SIFT | Data selection | Per-token compute |
| Official TTT | Fixed compute | Adaptive allocation |

---

## 6. ê¸°ëŒ€ ê¸°ì—¬

### 6.1 í•™ìˆ ì  ê¸°ì—¬

**Contribution 1: Adaptive TTT Framework**
- TTTì— ì²˜ìŒìœ¼ë¡œ adaptive compute ì ìš©
- REINFORCEë¡œ optimal allocation í•™ìŠµ

**Contribution 2: Temporal Credit Assignment**
- Sequential dependencyë¥¼ ê³ ë ¤í•œ policy learning
- Monte Carlo returns with discount factor

**Contribution 3: Comprehensive Evaluation**
- Multiple baselines (uniform, heuristic, learned)
- Oracle analysis for upper bound
- Statistical validation

### 6.2 ì‹¤ìš©ì  ê°€ì¹˜ (í˜„ì‹¤ì  í‰ê°€)

**âš ï¸ ì¤‘ìš”**: ì•„ë˜ ê¸°ëŒ€ì¹˜ëŠ” ë‚™ê´€ì ì´ë©°, null result ê°€ëŠ¥ì„±ë„ ê³ ë ¤í•´ì•¼ í•¨

**Efficiency gain (ë¶ˆí™•ì‹¤)**:
- ì´ìƒì  ì‹œë‚˜ë¦¬ì˜¤: 5-10% net FLOPs ì ˆê°
- í˜„ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤: Policy overheadê°€ ì ˆê°ë¶„ ìƒì‡„ (0% gain)
- ìµœì•… ì‹œë‚˜ë¦¬ì˜¤: Net negative (policy ë¹„ìš© > ì ˆê°ë¶„)
- **Policy overhead**: BiLSTM (14.2M params) ì‹¤í–‰ ë¹„ìš© ë¬´ì‹œ ë¶ˆê°€

**Quality improvement (ë¶ˆí™•ì‹¤)**:
- ì´ìƒì  ì‹œë‚˜ë¦¬ì˜¤: Marginal improvement (< 1% perplexity)
- í˜„ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤: Uniform K=4ì™€ í†µê³„ì ìœ¼ë¡œ ì°¨ì´ ì—†ìŒ
- **Null hypothesis ê°€ëŠ¥ì„±**: ëª¨ë“  í† í°ì´ ì‹¤ì œë¡œ ë¹„ìŠ·í•œ ê³„ì‚° í•„ìš”í•  ìˆ˜ ìˆìŒ

**Interpretability (í™•ì‹¤)**:
- ì–´ë–¤ í† í°ì´ ì–´ë ¤ìš´ì§€ policyê°€ í•™ìŠµ (ì„±ê³µ ì—¬ë¶€ì™€ ë¬´ê´€)
- Model debugging ë° ë¶„ì„ì— ë„ì›€
- ì´ê²ƒë§Œìœ¼ë¡œë„ ì—°êµ¬ ê°€ì¹˜ ìˆìŒ

---

## 7. í•œê³„ ë° í–¥í›„ ì—°êµ¬

### 7.1 í˜„ì¬ í•œê³„

**Dataset scale** (âš ï¸ Major):
- WikiText-2ëŠ” ë„ˆë¬´ ì‘ìŒ (2M tokens only)
- Policy í•™ìŠµì— ì¶©ë¶„í•œ ë°ì´í„°ì¸ê°€ ë¶ˆí™•ì‹¤
- WikiText-103 (100M tokens) ìµœì†Œ í•„ìš”

**Model size**:
- 60M parametersëŠ” ì‘ì€ í¸
- Larger modelì—ì„œ íš¨ê³¼ ê²€ì¦ í•„ìš”

**Domain**:
- Language modeling only
- Other domains (vision, speech) ê²€ì¦ í•„ìš”

**Policy overhead** (ğŸ”´ Critical):
- HaltingPolicyNetwork: 14.2M parameters (ì „ì²´ ëª¨ë¸ì˜ 23.6%)
- ë§¤ í† í°ë§ˆë‹¤ BiLSTM forward pass í•„ìš”
- Overheadê°€ ì ˆê°ë¶„ì„ ìƒì‡„í•  ê°€ëŠ¥ì„± ë†’ìŒ
- **Net gainì´ negativeì¼ ìˆ˜ ìˆìŒ**

### 7.2 ì£¼ìš” ìš°ë ¤ì‚¬í•­ ë° ë¦¬ìŠ¤í¬

**ğŸ”´ Critical Risks** (í”„ë¡œì íŠ¸ ì‹¤íŒ¨ ê°€ëŠ¥ì„±):

**1. Policy overhead > Savings**
- **ë¬¸ì œ**: BiLSTM ì‹¤í–‰ ë¹„ìš©ì´ K ì ˆê°ìœ¼ë¡œ ì–»ëŠ” ì´ë“ë³´ë‹¤ í´ ìˆ˜ ìˆìŒ
- **ë¶„ì„**:
  - Policy forward: ~14M params Ã— í† í°ë‹¹
  - TTT iteration savings: K ê°ì†Œë¶„ Ã— TTT params
  - Net gain = Savings - Overhead (ìŒìˆ˜ ê°€ëŠ¥)
- **ì™„í™”**:
  - Lightweight policy (MLP-only, no LSTM)
  - Amortized policy (mini-batch level)
  - Oracle ë¶„ì„ìœ¼ë¡œ upper bound ë¨¼ì € í™•ì¸

**2. ë¹„í˜„ì‹¤ì  ê¸°ëŒ€ì¹˜**
- **ë¬¸ì œ**: 15-30% ì ˆê°ì€ ê³¼ë„í•˜ê²Œ ë‚™ê´€ì 
- **í˜„ì‹¤**:
  - Policy overhead ê³ ë ¤ ì‹œ 0-5% ë˜ëŠ” negative
  - Null result ê°€ëŠ¥ì„± ë†’ìŒ
- **ëŒ€ì‘**:
  - ë³´ìˆ˜ì  ëª©í‘œ ì„¤ì • (5% net gain)
  - Negative resultë„ í•™ìˆ ì  ê¸°ì—¬ë¡œ ì¸ì •

**âš ï¸ Major Concerns** (ì—°êµ¬ ë‚œì´ë„):

**3. WikiText-2 ë„ˆë¬´ ì‘ìŒ**
- **ë¬¸ì œ**: 2M tokensë¡œ policy í•™ìŠµ ì–´ë ¤ì›€
- **ì¦ìƒ**: Overfitting, ë¶ˆì•ˆì •í•œ training
- **í•´ê²°**: WikiText-103 (100M tokens) í•„ìˆ˜

**4. REINFORCE ë¶ˆì•ˆì •**
- **ë¬¸ì œ**: High variance gradients
- **ì¦ìƒ**:
  - Trainingì´ ìˆ˜ë ´í•˜ì§€ ì•ŠìŒ
  - Policyê°€ degenerate solution í•™ìŠµ (í•­ìƒ K=1 ë˜ëŠ” K=8)
  - Reward signal ë„ˆë¬´ sparse
- **ì™„í™”**:
  - Strong baseline (value network)
  - Entropy regularization
  - Curriculum learning (easy â†’ hard)
  - PPO ê³ ë ¤

**5. Null Hypothesis ê°€ëŠ¥ì„±**
- **ë¬¸ì œ**: í•µì‹¬ ê°€ì„¤ì´ í‹€ë¦´ ìˆ˜ ìˆìŒ
- **ê°€ì„¤**: "í† í°ë³„ë¡œ ë‹¤ë¥¸ ê³„ì‚° í•„ìš”"
- **ë°˜ë¡€**: ì‹¤ì œë¡œ ëª¨ë“  í† í°ì´ ë¹„ìŠ·í•œ K í•„ìš”í•  ìˆ˜ ìˆìŒ
- **Oracle ë¶„ì„ ê²°ê³¼ê°€ uniform distributionì´ë©´?**
  - ì´ê²ƒë„ ì¤‘ìš”í•œ negative result
  - "Adaptive allocationì€ íš¨ê³¼ ì—†ë‹¤" ì¦ëª…
  - í•™ìˆ ì  ê¸°ì—¬ ì—¬ì „íˆ ì¡´ì¬

**ì—°êµ¬ ì„±ê³µ/ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤**:

| ì‹œë‚˜ë¦¬ì˜¤ | Net Gain | í•™ìˆ ì  ê°€ì¹˜ | ì‹¤ìš©ì  ê°€ì¹˜ |
|---------|----------|------------|------------|
| Best case | +5~10% | ë†’ìŒ | ë†’ìŒ |
| Good case | +1~5% | ë†’ìŒ | ì¤‘ê°„ |
| Null result | 0% | ì¤‘ê°„ | ë‚®ìŒ |
| Negative | < 0% | ë‚®ìŒ | ì—†ìŒ |

**Null result ëŒ€ë¹„ ì „ëµ**:
- Oracle analysisë¥¼ ë¨¼ì € ìˆ˜í–‰ (upper bound í™•ì¸)
- Heuristic baselinesë¡œ feasibility ê²€ì¦
- Interpretabilityì— ì§‘ì¤‘ (efficiency ëª» ì–»ì–´ë„ ë¶„ì„ ë„êµ¬ë¡œ ê°€ì¹˜)

### 7.3 í–¥í›„ ì—°êµ¬ ë°©í–¥

**Direction 1: Scaling**
- WikiText-103, C4, The Pile
- 1B+ parameter models
- Multi-domain evaluation

**Direction 2: Advanced Policies**
- Transformer-based policy
- Multi-granularity (token + layer)
- Learned adaptive mini-batch size

**Direction 3: Theoretical Analysis**
- Convergence guarantees
- Sample complexity bounds
- Optimal allocation theory

**Direction 4: Hybrid Approaches**
- Combine heuristic + learned
- Multi-stage allocation
- Dynamic threshold adaptation

**Direction 5: Other Architectures**
- Adaptive compute for Mamba
- Adaptive compute for RWKV
- Generalization to linear RNNs

---

## 8. í•µì‹¬ ì¸ì‚¬ì´íŠ¸

### 8.1 Why Adaptive Compute for TTT?

**Observation**: TTT updates fast-weight at every token

**Problem**: Not all tokens need same amount of updating

**Insight**:
- Easy tokens: W is already good, 1-2 steps enough
- Hard tokens: W needs more refinement, 4-8 steps needed

**Benefit**: Allocate compute where it matters most

### 8.2 Why REINFORCE?

**Alternative 1**: Fixed heuristics (entropy, loss)
- Pro: Simple, no training
- Con: Not optimal, no adaptation

**Alternative 2**: Differentiable gating (Gumbel-softmax)
- Pro: End-to-end differentiable
- Con: Biased gradients, complex

**REINFORCE choice**:
- Pro: Unbiased gradients, flexible
- Pro: Natural for discrete actions (K âˆˆ {1,2,4,8})
- Con: High variance (mitigate with baseline)
- âš ï¸ **ì£¼ì˜**: Training ë¶ˆì•ˆì •ì„± ì˜ˆìƒ, PPO ê³ ë ¤ í•„ìš”

### 8.3 Why Temporal Credit?

**Key insight**: Sequential dependency in TTT

**Mechanism**:
- Token t with K=8 â†’ Better W learned
- Better W â†’ Token t+1 benefits (even with K=1)

**Implication**: Policy must consider long-term effects

**Solution**: Monte Carlo returns with Î³=0.99

---

## 9. ë°©ë²•ë¡ ì  ì„ íƒì˜ ê·¼ê±°

### 9.1 Discrete K âˆˆ {1, 2, 4, 8}

**Why discrete?**
- Hardware efficiency (powers of 2)
- Easier to profile and optimize
- Clear semantic meaning

**Why these values?**
- K=1: Minimal compute baseline
- K=2, K=4: Intermediate options
- K=8: Maximum reasonable compute
- Covers wide range (1Ã— to 8Ã—)

### 9.2 Per-token (not per-layer)

**Why per-token?**
- TTT processes tokens sequentially
- Each token has different difficulty
- Finer granularity than per-layer

**Alternative**: Per-mini-batch
- Official TTT uses mini-batch
- PonderTTT uses per-token for adaptivity

### 9.3 REINFORCE (not other RL)

**Why not Q-learning?**
- Discrete action space (4 options)
- Policy gradient more natural

**Why not PPO?**
- Overkill for this problem
- REINFORCE + baseline sufficient

**Why not A3C?**
- Single sequence, not parallel envs
- No need for actor-critic

---

## 10. í‰ê°€ ë°©ë²•ë¡  ì„¤ê³„

### 10.1 Fairness in Comparison

**Challenge**: Different methods use different compute

**Solution 1**: Match average K
- Uniform K=4 vs Learned (target=4)
- Same expected FLOPs

**Solution 2**: Pareto frontier
- Plot all methods on efficiency-quality space
- Find dominating solutions

**Metrics**:
- Perplexity (quality)
- FLOPs (efficiency)
- Pareto optimality

### 10.2 Statistical Rigor

**Multiple seeds**: 10+ for variance estimation

**Paired tests**: Same data, different methods

**Corrections**: Bonferroni for multiple comparisons

**Effect size**: Cohen's d, not just p-values

**Bootstrap CI**: Non-parametric confidence intervals

### 10.3 Ablation Studies

**Essential ablations**:
1. Î³ ablation: {0.0, 0.5, 0.9, 0.99}
2. Î» ablation: {0.0, 0.01, 0.05, 0.1}
3. Policy architecture: LSTM vs MLP
4. Step options: {1,2,4,8} vs {1,4} vs {1,2,4,8,16}

---

## 11. ì¬êµ¬í˜„ ì‹œ ê³ ë ¤ì‚¬í•­

### 11.1 í•µì‹¬ ê²°ì • ì‚¬í•­

**Decision 1**: Iterative vs Analytical TTT?
- Current: Iterative (K-step GD)
- Alternative: Analytical with adaptive mini-batch size
- Tradeoff: Flexibility vs theoretical guarantees

**Decision 2**: Policy granularity?
- Current: Per-token
- Alternative: Per-position (averaged over batch)
- Tradeoff: Adaptivity vs stability

**Decision 3**: Discount factor Î³?
- Current: 0.99 (fixed)
- Alternative: Learned, position-dependent
- Requires: Ablation to validate choice

**Decision 4**: Step options?
- Current: {1, 2, 4, 8}
- Alternative: {1, 4, 16}, {2, 4, 8, 16}
- Requires: Analysis of tradeoff

### 11.2 í•„ìˆ˜ ê²€ì¦ ì‹¤í—˜

**ì‹¤í—˜ 0** (ğŸ”´ ìµœìš°ì„ ): **Policy overhead ì¸¡ì •**
- Policy forward pass FLOPs ì •í™•íˆ ì¸¡ì •
- TTT iteration FLOPs ì¸¡ì •
- Net gain ê³„ì‚°: Savings - Overhead
- **ì´ê²ƒì´ negativeë©´ í”„ë¡œì íŠ¸ ì¤‘ë‹¨ ê³ ë ¤**

**ì‹¤í—˜ 1**: **Oracle analysis** (Upper bound)
- ê° í† í°ì— ëŒ€í•´ K âˆˆ {1,2,4,8} ëª¨ë‘ ì‹œë„
- Best K ë¶„í¬ í™•ì¸
- Oracleì´ uniform distributionì´ë©´ null hypothesis í™•ì¸ë¨
- **Learned policyë³´ë‹¤ ë¨¼ì € ìˆ˜í–‰**

**ì‹¤í—˜ 2**: Convergence analysis
- Measure K-step iterative vs analytical gap
- Find minimum K for <1% gap
- Validate that K=4 or K=8 is sufficient

**ì‹¤í—˜ 3**: Gamma ablation
- Compare Î³ âˆˆ {0.0, 0.5, 0.9, 0.99}
- Measure sequential dependency strength
- Justify Î³=0.99 choice

**ì‹¤í—˜ 4**: Difficulty correlation
- Compute oracle K for each token
- Measure correlation with entropy, loss, gradient
- Validate difficulty metrics

**ì‹¤í—˜ 5**: Scaling
- WikiText-2 â†’ WikiText-103
- 60M â†’ 250M+ parameters
- Verify findings hold at scale

**ì‹¤í—˜ ìˆœì„œ** (ì¤‘ìš”):
1. Oracle analysis (feasibility í™•ì¸)
2. Policy overhead ì¸¡ì • (net gain ê°€ëŠ¥ì„± í™•ì¸)
3. Heuristic baselines (ê°„ë‹¨í•œ ë°©ë²•ìœ¼ë¡œ baseline í™•ë¦½)
4. Learned policy (ë§ˆì§€ë§‰ì— ì‹œë„)

### 11.3 ë¬¸ì„œí™” ì›ì¹™

**Honest claims only**:
- State what was validated
- Acknowledge what wasn't
- Clear about limitations

**Reproducibility**:
- All hyperparameters documented
- Seeds fixed and reported
- Environment specified

**Comparisons**:
- Clear about what is being compared
- Apples-to-apples (same algorithm variants)
- Fair baselines

---

## 12. ê²°ë¡ 

### í•µì‹¬ ì•„ì´ë””ì–´ ìš”ì•½

PonderTTTëŠ” **"ëª¨ë“  í† í°ì´ ë™ì¼í•œ ê³„ì‚°ì„ í•„ìš”ë¡œ í•˜ì§€ ì•ŠëŠ”ë‹¤"**ëŠ” ê´€ì°°ì—ì„œ ì¶œë°œí•˜ì—¬, **ê°•í™”í•™ìŠµì„ í†µí•´ í† í°ë³„ ìµœì  gradient descent ë°˜ë³µ íšŸìˆ˜ë¥¼ í•™ìŠµ**í•˜ëŠ” ë°©ë²•ë¡ ì…ë‹ˆë‹¤.

**Three pillars**:
1. **Adaptive allocation**: Per-token K âˆˆ {1,2,4,8}
2. **REINFORCE learning**: Policy gradient with baseline
3. **Temporal credit**: Monte Carlo returns (Î³=0.99)

### ê¸°ëŒ€ íš¨ê³¼ (í˜„ì‹¤ì  í‰ê°€)

**âš ï¸ ì¤‘ìš”í•œ ë©´ì±…**: ì•„ë˜ëŠ” ì´ìƒì  ì‹œë‚˜ë¦¬ì˜¤ì´ë©°, null result ê°€ëŠ¥ì„± ë†’ìŒ

**Efficiency (ë¶ˆí™•ì‹¤)**:
- ì´ìƒì : 5-10% net FLOPs reduction
- í˜„ì‹¤ì : 0% (policy overhead = savings)
- ìµœì•…: Negative (overhead > savings)

**Quality (ë¶ˆí™•ì‹¤)**:
- ì´ìƒì : Marginal perplexity improvement (< 1%)
- í˜„ì‹¤ì : No significant difference from Uniform K=4

**Interpretability (í™•ì‹¤)**:
- Learned difficulty assessment (ì„±ê³µ ì—¬ë¶€ì™€ ë¬´ê´€)
- ì´ê²ƒë§Œìœ¼ë¡œë„ ì—°êµ¬ ê°€ì¹˜ ìˆìŒ

### í›„ì† ì‘ì—… í•„ìš”

1. âœ… Convergence analysis (iterative vs analytical)
2. âœ… Gamma ablation (validate temporal credit)
3. âœ… Large-scale validation (WikiText-103, larger models)
4. âœ… Honest comparison framework
5. âœ… Statistical rigor (effect sizes, multiple seeds)

---

**ì´ ë¬¸ì„œëŠ” PonderTTTì˜ í•µì‹¬ ì•„ì´ë””ì–´ì™€ ë°©ë²•ë¡ ì„ ê¸°ë¡í•©ë‹ˆë‹¤.**
**ì½”ë“œ ì¬ì‘ì„± ì‹œ ì´ ì•„ì´ë””ì–´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ë˜, ë°œê²¬ëœ ë¬¸ì œë“¤ì„ í•´ê²°í•œ ìƒˆë¡œìš´ êµ¬í˜„ì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.**

**âš ï¸ ì¤‘ìš”**: ì´ ì—°êµ¬ëŠ” ë†’ì€ ì‹¤íŒ¨ ê°€ëŠ¥ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. Null/negative resultë„ í•™ìˆ ì ìœ¼ë¡œ ê°€ì¹˜ ìˆëŠ” ê¸°ì—¬ì„ì„ ì¸ì •í•˜ê³  ì‹œì‘í•´ì•¼ í•©ë‹ˆë‹¤.

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-11-09 (ì£¼ìš” ìš°ë ¤ì‚¬í•­ ë°˜ì˜)
