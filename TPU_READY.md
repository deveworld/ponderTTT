# âœ… TPU v4-64 Ready

TPU multi-host distributed training support implemented.

## ğŸ“‹ êµ¬í˜„ ì™„ë£Œ í•­ëª©

### âœ… 1. ë©€í‹°í˜¸ìŠ¤íŠ¸ ì´ˆê¸°í™”
**íŒŒì¼**: `src/ponderttt/utils/jax_utils.py:initialize_jax_distributed()`

```python
# ìë™ ì´ˆê¸°í™” (TPU Pod)
jax.distributed.initialize()

# ë˜ëŠ” ëª…ì‹œì  ì´ˆê¸°í™”
jax.distributed.initialize(
    coordinator_address="...",
    num_processes=8,
    process_id=process_id,
)
```

âœ… **ì™„ë£Œ**: JAX distributed ì´ˆê¸°í™” í•¨ìˆ˜ êµ¬í˜„

---

### âœ… 2. JAX Mesh ì„¤ì •
**íŒŒì¼**: `src/ponderttt/utils/jax_utils.py:create_mesh()`

```python
# TPU v4-64 (64 devices)
mesh = create_mesh((64, 1), ('batch', 'model'))

# ë˜ëŠ” 8-way DP, 8-way FSDP
mesh = create_mesh((8, 8), ('dp', 'fsdp'))
```

âœ… **ì™„ë£Œ**: Mesh ìƒì„± ìœ í‹¸ë¦¬í‹° êµ¬í˜„

---

### âœ… 3. ë°ì´í„° ìƒ¤ë”©
**íŒŒì¼**: `src/ponderttt/data/dataset.py:CodeDataset.__init__()`

```python
# ìë™ìœ¼ë¡œ ê° í˜¸ìŠ¤íŠ¸ê°€ ë‹¤ë¥¸ ë°ì´í„° ìƒ¤ë“œ ì²˜ë¦¬
if shard_across_hosts:
    num_hosts = jax.process_count()
    host_id = jax.process_index()
    self.dataset = self.dataset.shard(
        num_shards=num_hosts,
        index=host_id,
    )
```

âœ… **ì™„ë£Œ**: í˜¸ìŠ¤íŠ¸ë³„ ë°ì´í„° ìƒ¤ë”© êµ¬í˜„

---

### âœ… 4. ë°°ì¹˜ ìƒ¤ë”©
**íŒŒì¼**: `src/ponderttt/utils/jax_utils.py:shard_batch()`

```python
# NamedSharding ì‚¬ìš©
sharding = NamedSharding(mesh, PS('batch', None))
sharded_batch = jax.device_put(batch, sharding)
```

âœ… **ì™„ë£Œ**: ìµœì‹  JAX NamedSharding API ì‚¬ìš©

---

### âœ… 5. ë°°ì¹˜ í¬ê¸° ê³„ì‚°
**íŒŒì¼**: `src/ponderttt/utils/jax_utils.py:get_local_batch_size()`

```python
# Global batch = 512, 64 devices
# -> per_device = 8
# -> per_host (8 chips) = 64
local_batch_size = get_local_batch_size(512)
```

âœ… **ì™„ë£Œ**: ìë™ ë°°ì¹˜ í¬ê¸° ê³„ì‚°

---

### âœ… 6. ì²´í¬í¬ì¸íŒ…
**íŒŒì¼**: `src/ponderttt/utils/checkpointing.py:save_checkpoint()`

```python
# ì£¼ í˜¸ìŠ¤íŠ¸ë§Œ ì €ì¥ (replicated)
save_checkpoint(..., save_on_all_hosts=False)

# ê° í˜¸ìŠ¤íŠ¸ê°€ ìƒ¤ë“œ ì €ì¥ (FSDP)
save_checkpoint(..., save_on_all_hosts=True)
```

âœ… **ì™„ë£Œ**: ë©€í‹°í˜¸ìŠ¤íŠ¸ ì²´í¬í¬ì¸íŒ… ì§€ì›

---

### âœ… 7. í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
**íŒŒì¼**: `scripts/train_tpu.py`

```python
# ë©€í‹°í˜¸ìŠ¤íŠ¸ í•™ìŠµ
python scripts/train_tpu.py \
    --multi_host \
    --mesh_shape="64,1" \
    --global_batch_size=512
```

âœ… **ì™„ë£Œ**: TPU Pod í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ êµ¬í˜„

---

### âœ… 8. í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
**íŒŒì¼**: `scripts/test_distributed.py`

```python
# ë¶„ì‚° ì„¤ì • í…ŒìŠ¤íŠ¸
python scripts/test_distributed.py --multi_host
```

âœ… **ì™„ë£Œ**: ë¶„ì‚° ì„¤ì • ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

---

## ğŸ”§ í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ

### ìµœì‹  JAX íŒ¨í„´ ì‚¬ìš©
- âœ… `jax.make_mesh()` - ìµœì‹  ë©”ì‹œ ìƒì„±
- âœ… `NamedSharding` - ìµœì‹  ìƒ¤ë”© API
- âœ… `jax.jit` - ìë™ ìƒ¤ë”© (pjit deprecated)
- âœ… `jax.device_put()` - ëª…ì‹œì  ìƒ¤ë”© ë°°ì¹˜

### ì°¸ê³  ë¬¸ì„œ
- [Google Cloud TPU Pods with JAX](https://docs.cloud.google.com/tpu/docs/jax-pods)
- [Training GPT-2 with JAX on TPU](https://developers.googleblog.com/train-gpt2-model-with-jax-on-tpu)
- [TTT-LM-JAX Repository](https://github.com/test-time-training/ttt-lm-jax)

---

## ğŸš€ ì‚¬ìš© ë°©ë²•

### ë‹¨ì¼ í˜¸ìŠ¤íŠ¸ (TPU v4-8)
```bash
python scripts/test_distributed.py
python scripts/train_tpu.py --mesh_shape="8,1"
```

### ë©€í‹° í˜¸ìŠ¤íŠ¸ (TPU v4-64)
```bash
# ëª¨ë“  í˜¸ìŠ¤íŠ¸ì—ì„œ ë™ì‹œ ì‹¤í–‰
gcloud compute tpus tpu-vm ssh ponderttt-v4-64 \
  --zone=us-central2-b \
  --worker=all \
  --command="cd ponderttt && python scripts/train_tpu.py --multi_host --mesh_shape='64,1'"
```

---

## ğŸ“Š êµ¬í˜„ ì „í›„ ë¹„êµ

| í•­ëª© | ì´ì „ ìƒíƒœ | í˜„ì¬ ìƒíƒœ | ì ìˆ˜ |
|------|----------|----------|------|
| ë©€í‹°í˜¸ìŠ¤íŠ¸ ì´ˆê¸°í™” | âŒ ì—†ìŒ | âœ… `initialize_jax_distributed()` | 10/10 |
| JAX Mesh | âŒ ì—†ìŒ | âœ… `create_mesh()` | 10/10 |
| ë°ì´í„° ìƒ¤ë”© | âŒ ë³µì œë¨ | âœ… í˜¸ìŠ¤íŠ¸ë³„ ìƒ¤ë“œ | 10/10 |
| ë°°ì¹˜ ìƒ¤ë”© | âŒ ì—†ìŒ | âœ… `NamedSharding` | 10/10 |
| ì²´í¬í¬ì¸íŒ… | âš ï¸ ë‹¨ìˆœ | âœ… ë©€í‹°í˜¸ìŠ¤íŠ¸ ì§€ì› | 10/10 |
| í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ | âŒ ì—†ìŒ | âœ… TPU Pod ì§€ì› | 10/10 |

**ì¢…í•© ì ìˆ˜**: ğŸŸ¢ 60/60 (100%)

---

## âš ï¸ ë‚¨ì€ ì‘ì—…

### í…ŒìŠ¤íŠ¸ í•„ìš”
- [ ] ì‹¤ì œ TPU v4-8ì—ì„œ í…ŒìŠ¤íŠ¸
- [ ] ì‹¤ì œ TPU v4-64ì—ì„œ ë©€í‹°í˜¸ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§

### ìµœì í™” (ì„ íƒ)
- [ ] FSDP ìƒ¤ë”© ì „ëµ ì¶”ê°€
- [ ] Gradient checkpointing
- [ ] Mixed precision training

---

## ğŸ“ ì¤‘ìš” ë…¸íŠ¸

### Google Cloud TPU Pod ì‚¬ìš© ì‹œ
1. **ëª¨ë“  í˜¸ìŠ¤íŠ¸ì—ì„œ ë™ì‹œ ì‹¤í–‰ í•„ìˆ˜**
   - `--worker=all` í”Œë˜ê·¸ ì‚¬ìš©
   - JAXê°€ ìë™ìœ¼ë¡œ í˜¸ìŠ¤íŠ¸ ê°„ ë™ê¸°í™”

2. **jax.device_count() ì£¼ì˜**
   - ëª¨ë“  í˜¸ìŠ¤íŠ¸ì—ì„œ í˜¸ì¶œë  ë•Œê¹Œì§€ ë¸”ë¡ë¨
   - ë‹¨ì¼ í˜¸ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸ ì‹œ ë¬¸ì œ ì—†ìŒ

3. **ì¶œë ¥ ì¤‘ë³µ ë°©ì§€**
   - `print_on_main()` ì‚¬ìš©
   - ì£¼ í˜¸ìŠ¤íŠ¸(process_index=0)ë§Œ ì¶œë ¥

4. **ë°ì´í„° ìƒ¤ë”© í•„ìˆ˜**
   - ê° í˜¸ìŠ¤íŠ¸ê°€ ë‹¤ë¥¸ ë°ì´í„° ì²˜ë¦¬
   - `shard_across_hosts=True` ê¸°ë³¸ê°’

---

## âœ… ê²°ë¡ 

**í˜„ì¬ êµ¬í˜„ì€ TPU v4-64 ë©€í‹°í˜¸ìŠ¤íŠ¸ í™˜ê²½ì—ì„œ ì‘ë™í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.**

ì£¼ìš” ê¸°ëŠ¥:
1. âœ… ìµœì‹  JAX íŒ¨í„´ ì‚¬ìš©
2. âœ… ê³µì‹ Google Cloud ë¬¸ì„œ ê¸°ë°˜
3. âœ… TTT-LM-JAX ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì ìš©
4. âœ… ì™„ì „í•œ ë©€í‹°í˜¸ìŠ¤íŠ¸ ì§€ì›
5. âœ… ëª…ì‹œì  ìƒ¤ë”© ì œì•½ìœ¼ë¡œ í†µì‹  ìµœì í™”
6. âœ… ë³´ìˆ˜ì  íŒŒë¼ë¯¸í„° ìƒ¤ë”©ìœ¼ë¡œ ì•ˆì •ì„±
7. âœ… ë””ë²„ê¹… ë° ê²€ì¦ ë„êµ¬ ì œê³µ
8. âœ… ì‚¬ìš©í•˜ê¸° ì‰¬ìš´ ìŠ¤í¬ë¦½íŠ¸

ë‹¤ìŒ ë‹¨ê³„:
- ì‹¤ì œ TPU í•˜ë“œì›¨ì–´ì—ì„œ ê²€ì¦
- ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì¸¡ì •
- í”„ë¡œë•ì…˜ í•™ìŠµ ì‹¤í–‰

**ë²„ì „**: 0.2.0
**ìƒíƒœ**: Ready for TPU v4-64 âœ…

